{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCD Professional Academy Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "[The National Centers for Environment Information](https://www.ncei.noaa.gov) maintains one of the most significant archives on Earth, managing more than 30 petabytes of data and information that spans the entire spectrum of Earth’s environmental systems and cycles with comprehensive oceanic, atmospheric, and geophysical data.\n",
    "\n",
    "The [Storm Events Database](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00773) is an integrated database of severe weather events across the United States from 1950 to this year, with information about a storm event's location, azimuth, distance, impact, and severity, including the cost of damages to property and crops, loss of life, injuries, property damage, disruption to commerce, etc.\n",
    "\n",
    "This dataset allows identify the most severe storms and damage caused by them, as define correlations between the features that compose the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset\n",
    "The datasets are provided via [bulk download](https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/).\n",
    "There are 3 files linked by the event ID number. Details, locations and fatalities\n",
    "\n",
    "* Events Details file: The storm event description and data captured\n",
    "* Events Location file: The storm location data\n",
    "* Event fatalities file: the fatalities related to the storm\n",
    "\n",
    "The full descripiotn of all features as described [here](https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Bulk-csv-Format.pdf). The datasets relevants to this project are Storm details and fatalities file with the set of features described below:\n",
    "\n",
    "### Event Details file\n",
    "Named as `StormEvents_details-ftp_v1.0_dYYYY_cYYYYMMdd.csv` where dYYYY = data year and cYYYYMMdd = file creation date \n",
    "\n",
    "* `begin_yearmonth`: Ex: 201212 (YYYYMM format) The year and month that the event began \n",
    "* `begin_day`: Ex: 31 (DD format) The day of the month that the event began \n",
    "* `begin_time`: Ex: 2359 (hhmm format) The time of day that the event began \n",
    "* `end_yearmonth` Ex: Ex: 201301 (YYYYMM format) The year and month that the event ended\n",
    "* `end_day` Ex: 01 (DD format) The day of the month that the event ended\n",
    "* `end_time` Ex: 0001 (hhmm format) The time of day that the event ended\n",
    "* `episode_id` Ex: 61280, 62777, 63250 ID assigned by NWS to denote the storm episode;\n",
    "* `event_id` Ex: 383097, 374427, 364175 ID assigned by NWS for each individual storm event contained within a storm episode; links\n",
    "the record with the same event in the storm_event_details, storm_event_locations and\n",
    "storm_event_fatalities tables (Primary database key field).\n",
    "* `state` Ex: GEORGIA, WYOMING, COLORADO The state name where the event occurred (no State ID’s are included here; State Name is spelled out in ALL CAPS).\n",
    "* `year` Ex: 2000, 2006, 2012 The four digit year for the event in this record.\n",
    "* `month_name` Ex: January, February, March The name of the month for the event in this record (spelled out; not abbreviated).\n",
    "* `event_type` Ex: Hail, Thunderstorm Wind, Snow, Ice (spelled out; not abbreviated)\n",
    "* `begin_date_time` Ex: 04/1/2012 20:48:00\n",
    "MM/DD/YYYY hh:mm:ss (24 hour time usually in LST)\n",
    "* `end_date_time` Ex: 04/1/2012 21:03:00\n",
    "MM/DD/YYYY hh:mm:ss (24 hour time usually in LST)\n",
    "* `injuries_direct` Ex: 1, 0, 56\n",
    "The number of injuries directly caused by the weather event.\n",
    "* `injuries_indirect` Ex: 0, 15, 87\n",
    "The number of injuries indirectly caused by the weather event.\n",
    "* `deaths_direct` Ex: 0, 45, 23\n",
    "The number of deaths directly caused by the weather event.\n",
    "* `deaths_indirect` Ex: 0, 4, 6\n",
    "The number of deaths indirectly caused by the weather event.\n",
    "* `damage_property` Ex: 10.00K, 0.00K, 10.00M\n",
    "The estimated amount of damage to property incurred by the weather event (e.g. 10.00K =\n",
    "$10,000; 10.00M = $10,000,000)\n",
    "* `damage_crops` Ex: 0.00K, 500.00K, 15.00M\n",
    "The estimated amount of damage to crops incurred by the weather event (e.g. 10.00K =\n",
    "$10,000; 10.00M = $10,000,000).\n",
    "* `magnitude` Ex: 0.75, 60, 0.88, 2.75\n",
    "The measured extent of the magnitude type ~ only used for wind speeds (in knots) and hail size\n",
    "(in inches to the hundredth).\n",
    "* `magnitude_type`: Ex: EG, MS, MG, ES\n",
    "EG = Wind Estimated Gust; \n",
    "ES = Estimated Sustained Wind; \n",
    "MS = Measured Sustained Wind;\n",
    "MG = Measured Wind Gust (no magnitude is included for instances of hail).\n",
    "* `tor_f_scale` Ex: EF0, EF1, EF2, EF3, EF4, EF5\n",
    "Enhanced Fujita Scale describes the strength of the tornado based on the amount and type of\n",
    "damage caused by the tornado. The F-scale of damage will vary in the destruction area;\n",
    "therefore, the highest value of the F-scale is recorded for each event.\n",
    "|F-sccale|\n",
    "|-----------------------------------------|\n",
    "|EF0 – Light Damage (40 – 72 mph)|\n",
    "|EF1 – Moderate Damage (73 – 112 mph)|\n",
    "|EF2 – Significant damage (113 – 157 mph)|\n",
    "|EF3 – Severe Damage (158 – 206 mph)|\n",
    "|EF4 – Devastating Damage (207 – 260 mph)|\n",
    "|EF5 – Incredible Damage (261 – 318 mph)|\n",
    "\n",
    "* `tor_length` Ex: 0.66, 1.05, 0.48\n",
    "Length of the tornado or tornado segment while on the ground (in miles to the tenth).\n",
    "* `tor_width` Ex: 25, 50, 2640, 10\n",
    "\n",
    "### Storm Data Fatality File \n",
    "Named `StormEvents_fatalities-ftp_v1.0_dYYYY_cYYYYMMdd.csv.gz` where dYYYY = data year and cYYYYMMdd = file creation date\n",
    "\n",
    "* `fatality_id` Ex: 17582, 17590, 17597, 18222\n",
    "ID assigned by NWS to denote the individual fatality that occurred)\n",
    "* `event_id` Ex: 383097, 374427, 364175\n",
    "ID assigned by NWS for each individual storm event contained within a storm episode; links the\n",
    "record with the same event in the storm_event_details, storm_event_locations and\n",
    "storm_event_fatalities tables (Primary database key field)\n",
    "* `fatality_type` Ex: D , I\n",
    "(D = Direct Fatality; I = Indirect Fatality; assignment of this is determined by NWS software;\n",
    "details below are from NWS Directve 10-1605 at\n",
    "http://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf, Section 2.6)\n",
    "* `fatality_date` Ex: 4/3/2012 00:00\n",
    "MM/DD/YYYY hh:mm (time is usually 00.00)\n",
    "* `fatality_age` Ex: 38, 25, 69, 54\n",
    "The age in years of the fatality (sometimes ‘null’ if unknown)\n",
    "* `fatality_sex` Ex: M, F\n",
    "The gender of the fatality (sometimes ‘null’ if unknown)\n",
    "* `fatality_location` Ex: UT, OU, MH, PS\n",
    "|Direct Fatality Location Table|\n",
    "|------------------------------|\n",
    "|BF Ball Field|\n",
    "|BO Boating|\n",
    "|BU Business|\n",
    "|CA Camping|\n",
    "|CH Church|\n",
    "|EQ Heavy Equip/Construction\\\n",
    "|GF Golfing|\n",
    "|IW In Water|\n",
    "|LS Long Span Roof|\n",
    "|MH Mobile/Trailer Home|\n",
    "|OT Other/Unknown|\n",
    "|OU Outside/Open Areas|\n",
    "|PH Permanent Home|\n",
    "|PS Permanent Structure|\n",
    "|SC School|\n",
    "|TE Telephone|\n",
    "|UT Under Tree|\n",
    "|VE Vehicle and/or Towed Trailer|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Downloading files\n",
    "\n",
    "> **This section takes a bit of time, please wait until it completes downloading all datases**.\n",
    "\n",
    "First all files needs to be downloaded from bulk download website. The website provides files from 1950 until current year. The library BeatifulSoup allows to scrape the website and download all csv datasets. The desired files have the extension as `.csv.gz` and a name pattern well defined. The files will be saved to datasets folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 content found for url https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles regex StormEvents_(details|fatalities).*\n",
      "Downloading 146 files to datasets folder\n",
      "All files downloaded!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_website_for_download_link(url, regex):\n",
    "    html_text = requests.get(bulk_url).text\n",
    "    soup = BeautifulSoup(html_text,'html.parser')\n",
    "    content = []\n",
    "    for tag in soup.find_all(href=re.compile(regex)):\n",
    "        tag_content = tag.get('href')\n",
    "        content.append(tag_content)\n",
    "    print(\"{} content found for url {} regex {}\".format(len(content),url,regex))\n",
    "    return content\n",
    "\n",
    "def download_files_to_folder(download_url,files,destination_folder):\n",
    "    print(\"Downloading {} files to {} folder\".format(len(files),destination_folder))\n",
    "    for file in files:\n",
    "        r = requests.get(\"{}/{}\".format(download_url,file),allow_redirects=True)\n",
    "        with open(\"{}/{}\".format(destination_folder,file), 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    print(\"All files downloaded!\")\n",
    "\n",
    "bulk_url = 'https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles'\n",
    "links = scrape_website_for_download_link(bulk_url,\"StormEvents_(details|fatalities).*\")\n",
    "download_files_to_folder(bulk_url,links,\"datasets\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading datasets into Pandas\n",
    "After downloading the datasets, they need to be loaded into Pandas Dataframe. As mentioned above, only a subset of features will be used.\n",
    "The strategy is to join the Storm details and fatalities dataset for each year, and append to a parent dataset that will hold all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11472/3852773624.py:16: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f, usecols=columns)\n",
      "/tmp/ipykernel_11472/3852773624.py:16: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f, usecols=columns)\n",
      "/tmp/ipykernel_11472/3852773624.py:16: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f, usecols=columns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# define range of years\n",
    "years = np.arange(1950,2023,1)\n",
    "\n",
    "#find datasets at datases folder based on regex\n",
    "def find_dataset(regex):\n",
    "    return glob.glob(\"datasets/{}\".format(regex))[0]\n",
    "\n",
    "# load dataset into pandas DF from gzip\n",
    "def load_df(file_path,columns):\n",
    "    with gzip.open(file_path) as f:\n",
    "        df = pd.read_csv(f, usecols=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_storm_dataset_data(pattern,columns):\n",
    "    full_df = pd.DataFrame()\n",
    "    for year in years:\n",
    "        details_rgx = pattern.format(year)\n",
    "        details_file_path = find_dataset(details_rgx)\n",
    "        partial_df = load_df(details_file_path,columns)\n",
    "        full_df = pd.concat([full_df,partial_df],axis=\"rows\")\n",
    "    return full_df\n",
    "\n",
    "\n",
    "def merge(df_tuple,column):\n",
    "    return pd.merge(df_tuple[0],df_tuple[1],how=\"left\",on=[column])\n",
    "  \n",
    "details_rgx = \"StormEvents_details*d{}*.csv.gz\"\n",
    "fatalities_rgx = \"StormEvents_fatalities*d{}*.csv.gz\"\n",
    "\n",
    "storm_details_columns = ['BEGIN_YEARMONTH','BEGIN_DAY','BEGIN_TIME','END_YEARMONTH','END_DAY','END_TIME','EPISODE_ID','EVENT_ID','STATE','YEAR','MONTH_NAME','EVENT_TYPE','INJURIES_DIRECT',\n",
    "                         'INJURIES_INDIRECT','DEATHS_DIRECT','DEATHS_INDIRECT','DAMAGE_PROPERTY','DAMAGE_CROPS','MAGNITUDE','MAGNITUDE_TYPE','TOR_F_SCALE','TOR_LENGTH','TOR_WIDTH']\n",
    "\n",
    "storm_fatalities_columns = ['FATALITY_ID','EVENT_ID','FATALITY_TYPE','FATALITY_DATE','FATALITY_AGE','FATALITY_SEX','FATALITY_LOCATION']\n",
    "\n",
    "storm_details_df = load_storm_dataset_data(details_rgx,storm_details_columns)\n",
    "storm_fatalities_df = load_storm_dataset_data(fatalities_rgx,storm_fatalities_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = merge((storm_details_df,storm_fatalities_df), \"EVENT_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1738389, 29)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* DOC/NOAA/NESDIS/NCEI > National Centers for Environmental Information, NESDIS, NOAA, U.S. Department of Commerce\n",
    "* The Severe Weather Data Inventory (SWDI): a Geospatial Database of Severe Weather Data at the NOAA National Centers for Environmental Information (NCEI)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
